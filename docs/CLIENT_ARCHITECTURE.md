# MCP Client Architecture & Integration Guide

## Overview

The MCP Client demonstrates how to integrate Local Large Language Models (LLMs) with Model Context Protocol servers to create intelligent applications that can use external tools. This implementation showcases a complete integration between Ollama-hosted models and our network utilities MCP server.

## Architecture Components

### ðŸ—ï¸ **System Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User/Client   â”‚    â”‚   MCP Client     â”‚    â”‚   MCP Server    â”‚
â”‚                 â”‚    â”‚   (Port 8001)    â”‚    â”‚   (Port 8000)   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ HTTP        â”‚ â”‚â”€â”€â”€â–¶â”‚ â”‚ FastAPI      â”‚ â”‚    â”‚ â”‚ FastAPI     â”‚ â”‚
â”‚ â”‚ Requests    â”‚ â”‚    â”‚ â”‚ Endpoints    â”‚ â”‚    â”‚ â”‚ Server      â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                  â”‚    â”‚                 â”‚
                       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ â”‚ ChatService  â”‚ â”‚â—€â”€â”€â–¶â”‚ â”‚ Tool        â”‚ â”‚
â”‚   Ollama        â”‚â—€â”€â”€â–¶â”‚ â”‚ (Orchestr.)  â”‚ â”‚    â”‚ â”‚ Execution   â”‚ â”‚
â”‚   LLM Server    â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚qwen2.5:7b-  â”‚ â”‚    â”‚ â”‚ MCPClient    â”‚ â”‚â—€â”€â”€â–¶â”‚ â”‚ Redis       â”‚ â”‚
â”‚ â”‚instruct     â”‚ â”‚    â”‚ â”‚ (HTTP)       â”‚ â”‚    â”‚ â”‚ Cache       â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### 1. **ChatService** - The Orchestrator

The `ChatService` class is the heart of the MCP client, coordinating between:
- **User Input Processing**: Analyzing user queries for tool requirements
- **LLM Communication**: Managing conversations with Ollama
- **Tool Execution**: Calling MCP server tools when needed
- **Response Generation**: Combining tool results with LLM capabilities

```python
class ChatService:
    def __init__(self):
        self.mcp_client = MCPClient(MCP_SERVER_URL)
        self.ollama_client = OllamaClient(OLLAMA_URL, OLLAMA_MODEL)
        self.available_tools = []
    
    async def process_message(self, message: str, session_id: str) -> ChatResponse:
        # 1. Check for direct tool usage patterns
        # 2. Generate LLM response if needed
        # 3. Execute tools if requested
        # 4. Return comprehensive response
```

### 2. **MCPClient** - Tool Interface

Handles all communication with the MCP server:

```python
class MCPClient:
    async def get_available_tools(self) -> List[ToolInfo]:
        # Discover available tools from MCP server
        
    async def call_tool(self, tool_name: str, arguments: Dict, session_id: str):
        # Execute specific tools on MCP server
```

### 3. **OllamaClient** - LLM Interface

Manages communication with the local Ollama server:

```python
class OllamaClient:
    async def generate(self, prompt: str, tools_context: str = "") -> str:
        # Generate responses using local LLM
        # Handle tool context integration
```

## MCP Integration Patterns

### ðŸŽ¯ **Pattern 1: Direct Tool Routing**

For predictable queries, the client bypasses LLM processing and directly routes to tools:

```python
async def _check_direct_tool_usage(self, message: str, session_id: str):
    # Time-related keywords â†’ get_network_time tool
    time_keywords = ["time", "date", "current time", "what time", "when is it", "ntp"]
    if any(keyword in message.lower() for keyword in time_keywords):
        result = await self.mcp_client.call_tool("get_network_time", {}, session_id)
        return formatted_response(result)
    
    # Network-related keywords â†’ ping_host tool
    ping_keywords = ["ping", "connectivity", "connect", "reach", "test"]
    if any(keyword in message.lower() for keyword in ping_keywords):
        hostname = extract_hostname(message)
        result = await self.mcp_client.call_tool("ping_host", {"hostname": hostname}, session_id)
        return formatted_response(result)
```

**Benefits:**
- âš¡ Fast response times
- ðŸŽ¯ Reliable tool execution
- ðŸ”„ Consistent behavior

### ðŸŽ¯ **Pattern 2: LLM-Guided Tool Usage**

For complex queries, the LLM decides which tools to use:

```python
def _build_prompt(self, user_message: str, tools_context: str = "") -> str:
    if tools_context:
        # Second pass: Generate final response with tool results
        return f"""Based on the tool results, provide a helpful answer:
        Tool Results: {tools_context}
        User Question: {user_message}"""
    else:
        # First pass: Analyze query and decide on tool usage
        return f"""Analyze this query and respond appropriately:
        User request: "{user_message}"
        
        Instructions:
        - For time/date questions: USE_TOOL:get_network_time:{{}}
        - For ping/connectivity: USE_TOOL:ping_host:{{"hostname": "HOST"}}
        - Otherwise: Provide helpful response"""
```

## Model Selection & Optimization

### ðŸ¤– **Why Qwen2.5:7b-instruct?**

After testing multiple models, Qwen2.5:7b-instruct proved optimal for MCP integration:

| Model | Instruction Following | Tool Usage Accuracy | Response Quality |
|-------|---------------------|-------------------|------------------|
| **Qwen2.5:7b-instruct** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| Llama 3.1:8b-instruct | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| Mistral:7b | â­â­â­ | â­â­ | â­â­â­â­ |
| Llama 3.2:3b-instruct | â­â­â­ | â­â­â­ | â­â­â­ |

**Key Advantages:**
- ðŸŽ¯ **Precise Tool Usage**: Consistently follows `USE_TOOL:name:args` format
- ðŸ§  **Smart Context Understanding**: Excellent at identifying when tools are needed
- âš¡ **Efficient Processing**: Good balance of capability vs. speed
- ðŸ”„ **Reliable Behavior**: Predictable responses across sessions

## Session Management

### ðŸ“ **Session Context Flow**

```python
class ChatRequest(BaseModel):
    message: str
    session_id: str = "default"  # Enables context tracking

# Session flow:
User Query â†’ Session ID â†’ Tool Execution â†’ Cached Results â†’ Response
     â†“            â†“              â†“             â†“            â†“
  "What time"  "user-123"   get_network_time   Redis    "Current time is..."
```

**Benefits:**
- ðŸ”„ **Context Preservation**: Related queries share context
- ðŸ’¾ **Caching Efficiency**: Results cached per session
- ðŸ“Š **Usage Tracking**: Monitor tool usage patterns
- ðŸŽ¯ **Personalization**: Session-specific optimizations

## Error Handling & Resilience

### ðŸ›¡ï¸ **Multi-Layer Error Handling**

```python
# Layer 1: Connection Resilience
async def health_check():
    ollama_status = "unknown"
    mcp_status = "unknown"
    try:
        # Test Ollama connection
        # Test MCP server connection
    except Exception:
        # Graceful degradation
    
# Layer 2: Tool Execution Safety
async def call_tool(self, tool_name, arguments, session_id):
    try:
        result = await self.mcp_client.call_tool(tool_name, arguments, session_id)
        return result
    except Exception as e:
        return {"status": "error", "result_data": {"error": str(e)}}

# Layer 3: Response Generation Fallback
async def generate(self, prompt: str):
    try:
        # Primary LLM call
    except httpx.TimeoutException:
        return "Request timed out. Please try again."
    except Exception:
        return "Error generating response. Please try again."
```

## Performance Optimizations

### âš¡ **Async Operations**

All operations are non-blocking:

```python
# Concurrent operations
async def process_message(self, message: str, session_id: str):
    # Can handle multiple requests simultaneously
    tool_response = await self._check_direct_tool_usage(message, session_id)
    llm_response = await self.ollama_client.generate(message)
    # No blocking operations
```

### ðŸ’¾ **Intelligent Caching**

MCP server provides caching for tool results:

```python
# Automatic caching in MCP server
cache_key = f"cache:prompt:{session_id}:{hash(prompt)}"
cached_response = await redis_client.get(cache_key)
if cached_response:
    return cached_response  # Skip tool execution
```

## Real-World Usage Examples

### ðŸ• **Time Queries**

```bash
curl -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What time is it?", "session_id": "demo"}'

# Response:
{
  "response": "The current time according to NTP server (NTP Server: pool.ntp.org) is: 2025-11-07 21:36:39 UTC",
  "tools_used": ["get_network_time"],
  "session_id": "demo",
  "timestamp": "2025-11-07T13:36:39.532530"
}
```

### ðŸŒ **Network Queries**

```bash
curl -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Can you ping github.com?", "session_id": "demo"}'

# Response:
{
  "response": "Ping test to github.com: Host Reachable. Connection successful with 25.2 ms average latency.",
  "tools_used": ["ping_host"],
  "session_id": "demo"
}
```

### ðŸ’¬ **General Conversation**

```bash
curl -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello, what can you help me with?", "session_id": "demo"}'

# Response:
{
  "response": "Hello! I can help with checking the current time and date, pinging hosts to test connectivity, or any other information you might need.",
  "tools_used": [],
  "session_id": "demo"
}
```

## MCP Standard Compliance

### ðŸ“‹ **How This Implementation Follows MCP Standards**

#### **1. Tool Discovery**
```python
# Standard MCP tool discovery
GET /v1/tools/list â†’ Returns tool schemas

# Client implementation
async def get_available_tools(self) -> List[ToolInfo]:
    response = await client.get(f"{self.base_url}/v1/tools/list")
    return [ToolInfo(**tool) for tool in response.json()]
```

#### **2. Tool Execution**
```python
# Standard MCP tool execution
POST /v1/tools/call â†’ Execute with structured request

# Client implementation
async def call_tool(self, tool_name: str, arguments: Dict, session_id: str):
    payload = {
        "tool_name": tool_name,
        "arguments": arguments,
        "session_id": session_id
    }
    response = await client.post(f"{self.base_url}/v1/tools/call", json=payload)
```

#### **3. Session Management**
```python
# Standard MCP session handling
class ToolCallRequest(BaseModel):
    tool_name: str
    arguments: Dict[str, Any]
    session_id: str  # Context preservation
```

#### **4. Error Handling**
```python
# Standard MCP error responses
class ToolCallResponse(BaseModel):
    status: str  # "success" or "error"
    result_data: Dict[str, Any]  # Results or error details
```

## Deployment Considerations

### ðŸš€ **Production Deployment**

#### **Resource Requirements**
- **CPU**: 4+ cores recommended for Qwen2.5:7b-instruct
- **RAM**: 8GB+ for model loading and concurrent requests
- **Storage**: 5GB+ for model files and caching
- **Network**: Low latency connection to MCP server

#### **Scaling Strategies**
- **Horizontal Scaling**: Multiple client instances behind load balancer
- **Model Optimization**: Quantized models for faster inference
- **Caching Layers**: Redis for tool results, model response caching
- **Connection Pooling**: Efficient HTTP client management

#### **Monitoring & Observability**
- **Health Endpoints**: Monitor all service dependencies
- **Performance Metrics**: Track response times and tool usage
- **Error Tracking**: Comprehensive error logging and alerting
- **Usage Analytics**: Session patterns and tool effectiveness

## Future Enhancements

### ðŸ”® **Planned Improvements**

#### **1. Advanced Tool Chaining**
```python
# Multiple tool usage in single request
async def handle_complex_query(self, message: str):
    # "What time is it in New York and can you ping nyt.com?"
    time_result = await self.call_tool("get_network_time", {})
    ping_result = await self.call_tool("ping_host", {"hostname": "nyt.com"})
    return combine_results(time_result, ping_result)
```

#### **2. Context-Aware Caching**
```python
# Intelligent cache invalidation
cache_key = f"context:{session_id}:{tool_name}:{hash(context)}"
# Time-sensitive caching for different tool types
```

#### **3. Multi-Model Support**
```python
# Dynamic model selection based on query complexity
async def select_optimal_model(self, query_complexity: int) -> str:
    if query_complexity > 8:
        return "qwen2.5:14b-instruct"  # Complex reasoning
    else:
        return "qwen2.5:7b-instruct"   # Standard queries
```

## Conclusion

This MCP client implementation demonstrates how to create sophisticated AI applications that combine the conversational abilities of local LLMs with the practical capabilities of external tools. By following MCP standards, the client ensures interoperability, maintainability, and extensibility while providing a seamless user experience.

The architecture balances performance, reliability, and ease of use, making it suitable for both development and production deployment. The modular design allows for easy adaptation to different models, tools, and use cases while maintaining the core MCP integration patterns.